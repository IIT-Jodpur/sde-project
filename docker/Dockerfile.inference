# Inference Workload Container
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

RUN pip3 install --upgrade pip

# Install inference-specific dependencies
RUN pip3 install \
    torch==2.0.1 \
    torchvision==0.15.2 \
    flask==2.3.2 \
    Pillow==9.5.0 \
    pynvml==11.5.0

WORKDIR /workspace

# Copy inference workloads
COPY workloads/inference/ /workspace/inference/

# Create necessary directories
RUN mkdir -p /workspace/results

# Expose port for inference server
EXPOSE 5000

# Default command runs inference server
CMD ["python3", "inference/inference_server.py", "--port", "5000"]

